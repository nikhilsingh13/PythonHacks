{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3196040-648f-4a94-a16b-31de082c5c3d",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "- Spam or no Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008a403-bcfd-4400-8608-30ef17c197a7",
   "metadata": {},
   "source": [
    "> The code below can be found in the book as well with minor changes...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3804e-be73-40aa-a211-cb4bd280f953",
   "metadata": {},
   "source": [
    "**The reason this technique is considered `naive` (or simplistic) is that it makes some simple yet extreme assumptions.**  \n",
    "- In probability terms, we say the events are mutually exclusive.\n",
    "\n",
    "**What are the events?**  \n",
    "- The occurrence of one word in the `email` (or any text) is assumed to have no connection with another word.\n",
    "\n",
    "Nowadays, this assumption can be outright rejected, but the point is: it's a simple yet powerful technique that utilises one of the key concepts of probability to make wise predictions.\n",
    "\n",
    "The formula representing the above assumption is:\n",
    "\n",
    "S - event: \"message is spam\".\n",
    "&\n",
    "contain the words like `bitcoin` & `rolex`..\n",
    "\n",
    "$$\n",
    "P(X_1 = x_1, \\ldots, X_n = x_n \\mid S) = P(X_1 = x_1 \\mid S) \\times \\cdots \\times P(X_n = x_n \\mid S)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982d33b-a63a-4a66-8918-b2fc87d6b543",
   "metadata": {},
   "source": [
    "> This model was quite popularly used as a Spam filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192f70b-693a-44b8-b211-29282f760e01",
   "metadata": {},
   "source": [
    "Using the same Bayes’ theorem logic as in the `bitcoin-only` spam filter, we can calculate the probability that a message is spam with this formula:\n",
    "\n",
    "$$\n",
    "P(S \\mid X = x) = \\frac{P(X = x \\mid S)}{P(X = x \\mid S) + P(X = x \\mid \\neg S)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93744e-e87e-477d-87cd-fa40fe456a39",
   "metadata": {},
   "source": [
    "-------\n",
    "Before we jump on to create our `spam filter`:\n",
    "\n",
    "Few more things to take care of:\n",
    "1. As there can be a lot of words in an email, to calculate probability for every vocabulary word and then multiplying them together can result into **underflow** problem.\n",
    "   > - Floating point numbers too close to 0.\n",
    "   > - Easier way to deal with them is to use `log`\n",
    "2. Calculate estimate for $P(Xi|S)$ and $P(Xi|¬S)$\n",
    "   > - probabilities that a spam message / non-spam message contains the word $w_i$.\n",
    "   > - To estimate probabilities like $P(X_i|S)$ and $P(X_i|\\neg S)$, we use the fraction of spam / nonspam messages containing the word $w_i$.\n",
    "However, if a word appears only in nonspam messages, we might estimate $P(w_i|S) = 0$, which causes issues for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082386d-1ab9-4259-8d2f-277b8b15ae0a",
   "metadata": {},
   "source": [
    "To fix this, we apply **smoothing** using a pseudocount $k$. \n",
    "\n",
    "The smoothed estimate becomes:\n",
    "\n",
    "$$P(X_i|S) =\\frac{k + ws}{2k+ ms}$$\n",
    "\n",
    "- $ws$ = number of spams containing word $w_i$\n",
    "- $ms$ = number of spam messages\n",
    "\n",
    "This prevents zero probabilities and ensures the classifier can handle rare words.\n",
    "\n",
    "Similarly for $P(X_i | \\neg S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233ee81-d4af-449f-9dbd-a554fa633c22",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "## Implementation\n",
    "\n",
    "### Tokenising messages into simple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0438d6c5-caea-4616-bbe2-9979f82ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from typing import Set, NamedTuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a069f8-1653-4b8c-bf3f-93c6a81f160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text: str) -> Set[str]:\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text) # word extraction\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80beb613-142e-4d7e-8c66-801cdfba423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenise(\"Machine learning is about making a machine learn\") == {'a', 'about', 'is', 'learn', 'learning', 'machine', 'making'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568100ca-1992-4f36-ba84-0685b895ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b92b55-ba03-4318-9b9b-32de0c239346",
   "metadata": {},
   "source": [
    "- it is a `class` to keep track of `tokens`, `counts` and `labels` from the training data.\n",
    "- non-spam emails will be categorised as `okay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fe64b-80df-482f-b5f9-7b65afee58f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
