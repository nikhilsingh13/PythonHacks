# -*- coding: utf-8 -*-
"""Ch9_GettingData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yRgvgHx49flMIpraAzhi3zUFzJNQyBtr

> Data Scientists spend an embarrassingly large fraction of time acquiring, cleaning and transforming data.

Tbh, that's my life as well. Organisations want to go the LLMs way but don't want to adapt any data culture.

### stdin and stdout
"""

# reads in lines of text and output the ones that match the regular expression

#egrep.py
import sys, re

# sys.argv is the list of command line arguments
# sys.argv[0] is the name of the program itself
# sys.argv[1] will be the regex specified at the command line

regex = sys.argv[1]

# for every line passed into the script
for line in sys.stdin:
  # if it matches the regex, write it to stdout
  if re.search(regex, line):
    sys.stdout.write(line)

# line_count.py
import sys
count=0
for line in sys.stdin:
  count += 1

# print goes to sys.stdout:
print(count)

# script to count the words in its input and writes out the most common ones:
# most_common_word.py

import sys
from collections import Counter

# pass in number of words as first argument
try:
  num_words = int(sys.argv[1])
except:
  print("usage: most_common_words.py num_words")
  sys.exit(1) # non zero exit code indicates error

counter = Counter(word.lower() # lowercase words
                  for line in sys.stdin
                  for wird in line.strip().split() # split on spaces
                  if word)

for word, count in counter.most_common(num_words):
  sys.stdout.write(str(count))
  sys.stdout.write("\t")
  sys.stdout.write(word)
  sys.stdout.write("\n")

# !curl -s https://www.gutenberg.org/files/11/11-0.txt | python most_common_words.py 10


# this script needs to run as a standalone script. Then use the `curl` command.

"""### Reading files"""

